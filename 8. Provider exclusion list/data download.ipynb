{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95157b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import re\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512581cd",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53846195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_links(soup,url):\n",
    "    \"\"\"\n",
    "    Find all links to CSV files on the page.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if any(href.lower().endswith(ext) for ext in ['.csv', '.pdf']):\n",
    "            links.append(urljoin(url, link['href']))\n",
    "    print(f\"Found {len(links)} links to files.\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dd305e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(links,path,csv = True, pdf = True):\n",
    "    \"\"\"\n",
    "    Download all files from the links found on the page.\n",
    "    \"\"\"\n",
    "    for url in links:\n",
    "        if csv:\n",
    "            if not url.endswith('.csv'):\n",
    "                continue\n",
    "        if pdf:\n",
    "            if not url.endswith('.pdf'):\n",
    "                continue\n",
    "        filename = os.path.basename(url)\n",
    "        file_path = os.path.join(path, filename)\n",
    "\n",
    "        print(f\"Downloading {filename} ...\")\n",
    "        try:\n",
    "            file_response = requests.get(url)\n",
    "            file_response.raise_for_status()\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(file_response.content)\n",
    "            print(f\"Saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "        time.sleep(1)  \n",
    "        \n",
    "    print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78536b",
   "metadata": {},
   "source": [
    "## montly exclude list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4d5b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_csv = \"https://oig.hhs.gov/exclusions/supplements.asp\"\n",
    "dowload_dir_csv = \"OIG_exclude_csv/raw\"\n",
    "\n",
    "os.makedirs(dowload_dir_csv, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90feb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_csv = requests.get(url_csv)\n",
    "response_csv.raise_for_status()\n",
    "\n",
    "soup_csv = BeautifulSoup(response_csv.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9560a12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33 links to files.\n",
      "All done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\木木小呆猪\\AppData\\Local\\Temp\\ipykernel_8576\\734379929.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  csv_list_new = pd.DataFrame(np.array(csv_links)).applymap(lambda x: x.lower())\n",
      "C:\\Users\\木木小呆猪\\AppData\\Local\\Temp\\ipykernel_8576\\734379929.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  csv_update = csv_list_new.applymap(lambda x: x not in csv_list_last.values)\n"
     ]
    }
   ],
   "source": [
    "# download all links\n",
    "csv_links = download_links(soup_csv, url_csv)\n",
    "# download/update files\n",
    "if len(os.listdir(dowload_dir_csv)) == 0:\n",
    "    download_files(csv_links, dowload_dir_csv)\n",
    "else:\n",
    "    csv_list_last_name = \"csv_list_last.csv\"\n",
    "    csv_list_last = pd.read_csv(csv_list_last_name)\n",
    "    last_update_time = csv_list_last.columns[0]\n",
    "\n",
    "    csv_list_new = pd.DataFrame(np.array(csv_links)).applymap(lambda x: x.lower())\n",
    "    csv_update = csv_list_new.applymap(lambda x: x not in csv_list_last.values)\n",
    "    csv_list_update = csv_list_new[csv_update[0] == True]\n",
    "\n",
    "    if len(csv_list_update) == 0:\n",
    "        print(\"No new files to download.\")\n",
    "    else:\n",
    "        download_files(list(csv_list_update.iloc[:,0]), dowload_dir_csv, csv = True, pdf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bcd2eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://oig.hhs.gov/exclusions/files/leie_updated_information.pdf']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(csv_list_update.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c53cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the list of csv files we already have\n",
    "update_time = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "pd.DataFrame(np.array(csv_links),columns = [str(update_time)]).applymap(lambda x: x.lower()).to_csv(\"OIG_exclude_csv/csv_list_last.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a69511",
   "metadata": {},
   "source": [
    "## LEIE Downloadable Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7806b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_LEIE = \"https://oig.hhs.gov/exclusions/exclusions_list.asp\"\n",
    "dowload_dir_LEIE = \"LEIE downloadable\"\n",
    "os.makedirs(dowload_dir_LEIE, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1e064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_LEIE = requests.get(url_LEIE)\n",
    "response_LEIE.raise_for_status()\n",
    "soup_LEIE = BeautifulSoup(response_LEIE.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a32436",
   "metadata": {},
   "source": [
    "If the last update of this webpage is later than that in our database, we need to download the raw data, \n",
    "otherwise create a new folder to store the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c6a5bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Update is 2025-05-09\n"
     ]
    }
   ],
   "source": [
    "# find the last update date in the page\n",
    "match = re.search(r'\\b(\\d{2}-\\d{2}-\\d{4})\\b', soup_LEIE.text)\n",
    "if match:\n",
    "    last_update_web = match.group(1)\n",
    "    last_update_web = datetime.datetime.strptime(last_update_web, \"%m-%d-%Y\").date()\n",
    "    print(\"Last Update is\", last_update_web)\n",
    "else:\n",
    "    print(\"did not find last update date in the page.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2ac0fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No need to update\n"
     ]
    }
   ],
   "source": [
    "# need to update or not\n",
    "last_update_local = os.listdir(dowload_dir_LEIE)[-1]\n",
    "last_update_local = datetime.datetime.strptime(last_update_local, \"%Y-%m-%d\").date()\n",
    "\n",
    "if last_update_web > last_update_local:\n",
    "    print(\"Need to update\")\n",
    "    os.makedirs(os.path.join(dowload_dir_LEIE, last_update_web), exist_ok=True)\n",
    "    path = os.path.join(dowload_dir_LEIE, str(last_update_web))\n",
    "    update_status = True\n",
    "else:\n",
    "    print(\"No need to update\")\n",
    "    update_status = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df631c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if update_status:\n",
    "    # find all links to CSV files on the page\n",
    "    file_links = download_links(soup_LEIE)\n",
    "    # download all files from the links found on the page\n",
    "    download_files(file_links, path, csv = True, pdf = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b3b70",
   "metadata": {},
   "source": [
    "## Georgia OIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adea51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_GA = \"https://dch.georgia.gov/office-inspector-general/georgia-oig-exclusions-list\"\n",
    "dowload_dir_GA = \"GA OIG exclude\"\n",
    "os.makedirs(dowload_dir_GA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a59047",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_GA = requests.get(url_GA)\n",
    "response_GA.raise_for_status()  # 如果请求失败，将引发异常\n",
    "\n",
    "soup_GA = BeautifulSoup(response_GA.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_links = []\n",
    "for a in soup_GA.find_all('a', href=True):\n",
    "    href = a['href']\n",
    "    if href.endswith('/download'):\n",
    "        full_url = urljoin(url_GA, href)\n",
    "        download_links.append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b39d77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://dch.georgia.gov/document/document/dch-oig-exclusions-list-05062025/download']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d38503b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file: https://dch.georgia.gov/document/document/dch-oig-exclusions-list-05062025/download\n",
      "Successfully saved: GA OIG exclude\\dch-oig-exclusions-list-05062025.xlsx\n"
     ]
    }
   ],
   "source": [
    "for i, url in enumerate(download_links):\n",
    "\n",
    "    path_parts = url.strip('/').split('/')\n",
    "    if len(path_parts) >= 2:\n",
    "        filename_base = path_parts[-2]\n",
    "    else:\n",
    "        filename_base = f\"file_{i}\"\n",
    "\n",
    "    filename = filename_base + \".xlsx\"\n",
    "    save_path = os.path.join(dowload_dir_GA, filename)\n",
    "\n",
    "    print(f\"Saving file: {url}\")\n",
    "    try:\n",
    "        r = requests.get(url, stream=True)\n",
    "        r.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully saved: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"fail to download: {url}\\n Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b636a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
